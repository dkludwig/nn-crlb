'''
Train neural network to invert a linear encoding.

The neural network we train to invert the 100x500
encoding matrix A (from "make_data.py") has
two fully-connected hidden layers, the first with
50,000 sigmoid neurons, the second with 5,000,
and then a linear output layer with 100 linear neurons.
The training minimizes the MSE on the 50,000 training
vectors generated by "make_data.py".

Hyperparameters:
- learning_rate = 0.05
- batch_size = 100
- momentum = 0.7

The learning rate of 0.05 was chosen with a good bit of
experimentation, but the batch size and momentum coefficient
were chosen basically heuristically. The training seems to flatline
at around 7 epochs, so we set it around to go through 10.

The full model is saved to a folder labeled "model"
in the current directory.
'''
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

def main():
    # load training and test data
    with np.load('nn-inverse-data.npz') as data:
        x_train = data['x_train']
        x_test = data['x_test']
        y_train = data['y_train']
        y_test = data['y_test']
        A = data['A']

    # initialize network architecture
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(50000, activation='sigmoid',
                              input_shape=(500,)),
        tf.keras.layers.Dense(5000, activation='sigmoid'),
        tf.keras.layers.Dense(100, activation='linear')
    ])

    # set hyperparameters and loss function
    model.compile(tf.keras.optimizers.SGD(learning_rate=0.05, momentum=0.7),
                  loss=tf.keras.losses.MeanSquaredError())

    # train model
    history = model.fit(x_train, y_train,
                     validation_data=(x_test, y_test),
                     epochs=10, verbose=1, batch_size=100)

    # save model
    model.save('model')

    # plot loss history
    fig, axs = plt.subplots(1,2,figsize=(8,4))
    epochs = range(1, len(history.history['loss'])+1)

    axs[0].plot(epochs, history.history['loss'], label='loss')
    axs[0].plot(epochs, history.history['val_loss'], label='validation loss')
    axs[0].set_xlabel('Epochs')
    axs[0].set_ylabel('Mean Squared Error')
    axs[0].legend()

    axs[1].plot(epochs, history.history['loss'], label='loss')
    axs[1].plot(epochs, history.history['val_loss'], label='validation loss')
    axs[1].set_xlabel('Epochs')
    axs[1].set_yscale('log')
    axs[1].legend()

    plt.show()

if __name__ == '__main__':
    main()
